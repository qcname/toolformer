{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07228e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345402d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d637dc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import re\n",
    "from typing import List,Callable,Tuple,Union,TypedDict\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchtyping import TensorType\n",
    "from einops import rearrange\n",
    "from toolformer.api import BaseApI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20796764",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AugmentedCandidate(TypedDict):\n",
    "    api_start_positions:int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c6e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: dict,\n",
    "        model: Callable, tokenizer: Callable,\n",
    "        apis: List[BaseApI],\n",
    "        #device: str = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        device=torch.device(\"cpu\")\n",
    "    ):\n",
    "        super().__init__()\n",
    "        start_character = config[\"data_generator\"][\"api_start_ch\"]\n",
    "        end_character = config[\"data_generator\"][\"api_end_ch\"]\n",
    "        output_character = config[\"data_generator\"][\"api_out_ch\"]\n",
    "        \n",
    "        # add a space, because when the model generate a token, it's also include a \"space\"\n",
    "        self.api_start_token_id = tokenizer(f' {start_character}', return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        self.api_end_token_id = tokenizer(end_character, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        self.api_output_token_id = tokenizer(f'{output_character}', return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        \n",
    "        self.top_k_sampling = config[\"data_generator\"][\"top_k_sampling\"]\n",
    "        self.sampling_threshold = config[\"data_generator\"][\"sampling_threshold\"]\n",
    "        self.filtering_threshold = config[\"data_generator\"][\"filtering_threshold\"]\n",
    "        \n",
    "        self.apis = apis\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        \n",
    "        # TODO: handle for cases that the sentence contains \".\\n\\n\"\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.eos_token_id = tokenizer(\".\\n\\n\")[\"input_ids\"][0]\n",
    "    \n",
    "    def sample_api_position(\n",
    "        self,\n",
    "        prompt_ids: TensorType[\"seq_len\"], # the ids of the prompt\n",
    "    ) -> Tuple[\n",
    "        TensorType[\"n_positions\"], # The positions of api call\n",
    "        TensorType[\"seq_len\"] # The generated text\n",
    "    ]:\n",
    "        \"\"\"Sampling API positions.\"\"\"\n",
    "        # TODO: add support batch\n",
    "        # the ids of the prompt and generated_ids\n",
    "        prompt_and_generated_ids = prompt_ids\n",
    "        # only the ids of the generated_ids\n",
    "        generated_ids = torch.tensor([]).to(self.device)\n",
    "        i = torch.tensor([0]).to(self.device)\n",
    "        \n",
    "        api_pos_probs = torch.tensor([])\n",
    "        \n",
    "        with torch.no_grad():    \n",
    "            while True:\n",
    "                logits = self.model(\n",
    "                    input_ids=prompt_and_generated_ids.unsqueeze(0),\n",
    "                ).logits\n",
    "\n",
    "                last_logit = logits[0, -1, :]\n",
    "                probs = torch.softmax(last_logit, dim=-1)\n",
    "                api_start_prob = probs[self.api_start_token_id]\n",
    "                \n",
    "                if api_start_prob > self.sampling_threshold:\n",
    "                    api_pos_probs = torch.cat([\n",
    "                        api_pos_probs,\n",
    "                        torch.tensor([api_start_prob, i]).unsqueeze(0)\n",
    "                    ], dim=0)     \n",
    "                \n",
    "                # sampling a token\n",
    "                # next_token = torch.multinomial(probs, num_samples=1)\n",
    "                next_token = torch.argmax(probs, dim=-1)\n",
    "                next_token = next_token.unsqueeze(0)\n",
    "                \n",
    "                prompt_and_generated_ids = torch.cat([prompt_and_generated_ids, next_token], dim=0)\n",
    "                generated_ids = torch.cat([generated_ids, next_token], dim=0)\n",
    "                \n",
    "                if next_token == self.eos_token_id:\n",
    "                    break\n",
    "                else:\n",
    "                    i += 1\n",
    "        \n",
    "        if api_pos_probs.numel() == 0:\n",
    "            api_positions = torch.tensor([])\n",
    "        else:\n",
    "            _, indices = torch.sort(api_pos_probs[:, 0], descending=True)\n",
    "            top_k_sampling = self.top_k_sampling\n",
    "            api_positions = api_pos_probs[indices[:top_k_sampling], 1]\n",
    "                    \n",
    "        return api_positions.long(), generated_ids.long()\n",
    "\n",
    "    def obtain_api_response(\n",
    "        self,\n",
    "        prompt_ids: TensorType[\"seq_len\"],\n",
    "        positions: TensorType[\"n_positions\"],\n",
    "        generated_ids: TensorType[\"seq_len\"]\n",
    "    ) -> TensorType[\"n_positions\", \"seq_len\"]:\n",
    "        \n",
    "        MAX_PAD = 50\n",
    "        \n",
    "        # the ids before the start of an api call\n",
    "        pre_api_ids = torch.tensor([])\n",
    "\n",
    "        for position in positions:\n",
    "            text_ids = torch.cat([generated_ids[:position], self.api_start_token_id], dim=0)\n",
    "            padded_text_ids = F.pad(text_ids, pad=(MAX_PAD - text_ids.shape[-1], 0), value=self.pad_token_id)\n",
    "            \n",
    "            pre_api_ids = torch.cat([\n",
    "                pre_api_ids,\n",
    "                rearrange(padded_text_ids, \"... -> 1 ...\")\n",
    "            ])\n",
    "        \n",
    "        PROMPT_LENGTH = len(prompt_ids)\n",
    "        \n",
    "        # TODO: optimzie this\n",
    "        prompt_and_pre_api_ids = torch.tensor([])\n",
    "        for x in pre_api_ids:\n",
    "            prompt_and_pre_api_ids = torch.cat([\n",
    "                prompt_and_pre_api_ids,\n",
    "                torch.cat([prompt_ids, x]).unsqueeze(0)\n",
    "            ], dim=0)\n",
    "                     \n",
    "        with torch.no_grad():\n",
    "            candidate_ids = self.model.generate(\n",
    "                input_ids=prompt_and_pre_api_ids.long(),\n",
    "                eos_token_id=self.eos_token_id,\n",
    "                max_new_tokens=50,\n",
    "            )\n",
    "        \n",
    "        # filter out the prompt template\n",
    "        # only keep the generated ids\n",
    "        candidate_ids = candidate_ids[:, PROMPT_LENGTH:]\n",
    "        \n",
    "        return candidate_ids\n",
    "    \n",
    "    def _generate_conditioning_prompts(\n",
    "        self,\n",
    "        api: BaseApI,\n",
    "        candidate_ids: TensorType[\"n_candidates\", \"seq_len\"],\n",
    "    ):\n",
    "        conditioning_api_ids = torch.tensor([])\n",
    "\n",
    "        API_NAME = api.name\n",
    "        MAX_PAD = 100\n",
    "        \n",
    "        def extract_api_request_content(text: str, api_name: str) -> str:\n",
    "            \"\"\"Extract the content of an API request from a given text.\"\"\"\n",
    "            start_tag = f\"{api_name}(\"\n",
    "            end_tag = \")\"\n",
    "            start_idx = text.find(start_tag)\n",
    "            if start_idx == -1:\n",
    "                return None\n",
    "            start_idx += len(start_tag)\n",
    "            end_idx = text.find(end_tag, start_idx)\n",
    "            if end_idx == -1:\n",
    "                return None\n",
    "            return text[start_idx:end_idx]\n",
    "        \n",
    "        def extract_api_syntax(text: str, api_name: str) -> str:\n",
    "            \"\"\"Extract the API Syntax from a given text.\"\"\"\n",
    "            pattern = r\"\\[{}\\(.*?\\)\\]\".format(api_name)\n",
    "            matches = re.findall(pattern, text)\n",
    "            return matches\n",
    "\n",
    "        for text_ids in candidate_ids:\n",
    "            # the ids of the prediction\n",
    "            text = self.tokenizer.decode(text_ids, skip_special_tokens=True)\n",
    "            \n",
    "            api_request_content = extract_api_request_content(text, api_name=API_NAME)\n",
    "            api_response = api(api_request_content)\n",
    "            api_response_ids = self.tokenizer(api_response, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "            # Format: \"-> [api_response]\"\n",
    "            api_response_with_arrow_ids = torch.cat([self.api_output_token_id, api_response_ids], dim=0)\n",
    "            \n",
    "            api_syntax = extract_api_syntax(text, api_name=API_NAME)\n",
    "            api_syntax_ids = self.tokenizer(api_syntax, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "            api_syntax_with_response_ids = torch.cat([api_syntax_ids[:-1], api_response_with_arrow_ids, api_syntax_ids[-1:]])\n",
    "            api_syntax_without_response_ids = torch.cat([api_syntax_ids[:-1], self.api_output_token_id, api_syntax_ids[-1:]])\n",
    "                              \n",
    "            padded_api_without_response = rearrange(\n",
    "                F.pad(api_syntax_without_response_ids, pad=((MAX_PAD - api_syntax_without_response_ids.shape[-1]), 0), value=self.pad_token_id),\n",
    "                \"... -> 1 ...\"\n",
    "            )\n",
    "            padded_api_with_response = rearrange(\n",
    "                F.pad(api_syntax_with_response_ids, pad=((MAX_PAD - api_syntax_with_response_ids.shape[-1]), 0), value=self.pad_token_id),\n",
    "                \"... -> 1 ...\"\n",
    "            )\n",
    "        \n",
    "            padded_api_call = torch.cat([\n",
    "                padded_api_without_response,\n",
    "                padded_api_with_response\n",
    "            ], dim=0)\n",
    "            padded_api_call = rearrange(padded_api_call, \"... -> 1 ...\")\n",
    "            \n",
    "            conditioning_api_ids = torch.cat([conditioning_api_ids, padded_api_call], dim=0).long()\n",
    "                    \n",
    "        return conditioning_api_ids\n",
    "\n",
    "    def _filter_candidate_by_threshold(\n",
    "        self,\n",
    "        losses,\n",
    "        candidates: TensorType[\"seq_len\"]\n",
    "    ):\n",
    "        filtered_augmented_text_ids = torch.tensor([])\n",
    "        for i, position in enumerate(losses):\n",
    "            negative_loss = min(losses[position][0], losses[position][1])\n",
    "            positive_loss = losses[position][2]\n",
    "            \n",
    "            if negative_loss - positive_loss >= self.filtering_threshold:\n",
    "                # filtered_augmented_text_ids.append(candidates[i])\n",
    "                filtered_augmented_text_ids = torch.cat([\n",
    "                    filtered_augmented_text_ids,\n",
    "                    candidates[i].unsqueeze(0)\n",
    "                ], dim=0)\n",
    "        \n",
    "        return filtered_augmented_text_ids.long()\n",
    "\n",
    "    def filter_api( \n",
    "        self,\n",
    "        api: BaseApI,\n",
    "        text_ids: TensorType[\"seq_len\"],\n",
    "        api_start_idxs: TensorType[\"n_positions\"],\n",
    "        candidate_ids: TensorType[\"n_positions\", \"seq_len\"]\n",
    "    ):\n",
    "        conditioning_api_ids = self._generate_conditioning_prompts(api, candidate_ids)\n",
    "                \n",
    "        SPACE_TOKEN = self.tokenizer(\". \", return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        API_LENGTH = 100\n",
    "        augmented_text_ids = {\"api_start_positions\": {}}\n",
    "        \n",
    "        def _compute_weight(t: int) -> Union[int, float]:\n",
    "            \"\"\"Compute the weight in the loss function.\"\"\"\n",
    "            return max(0, 1-0.2*t)\n",
    "        \n",
    "        for idx, api_ids in zip(api_start_idxs, conditioning_api_ids):\n",
    "            idx = idx.item()\n",
    "            seq_len = len(text_ids)\n",
    "            augmented_text_ids[\"api_start_positions\"][idx] = {\n",
    "                \"seq_positions\": {}\n",
    "            }\n",
    "\n",
    "            j = idx\n",
    "            while j <= seq_len - 1:\n",
    "                # if the model predic\n",
    "                if j == 1:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                \n",
    "                # in the formua, from x_1 to x_j (include x_j)\n",
    "                # => generate_ids[:j]\n",
    "                conditioning_text_ids = text_ids[:j]\n",
    "                api_and_text_ids = torch.stack([\n",
    "                    F.pad(conditioning_text_ids, pad=(API_LENGTH + len(SPACE_TOKEN), 0), value=self.pad_token_id), # [text_ids]\n",
    "                    torch.cat([api_ids[0], SPACE_TOKEN, conditioning_text_ids], dim=0), # [api->, text_ids]\n",
    "                    torch.cat([api_ids[1], SPACE_TOKEN, conditioning_text_ids], dim=0), # [api->result, text_ids]\n",
    "                ], dim=0)\n",
    "                                \n",
    "                # the next token after x_j\n",
    "                next_token_ids = text_ids[j]\n",
    "                augmented_text_ids[\"api_start_positions\"][idx][\"seq_positions\"][j] = {\n",
    "                    \"prompt_ids\": api_and_text_ids,\n",
    "                    \"unnormalized_weight\": _compute_weight(t=j-idx),\n",
    "                    \"losses\": [],\n",
    "                    \"target_ids\": torch.tensor([next_token_ids, next_token_ids, next_token_ids])\n",
    "                }\n",
    "                j += 1\n",
    "        \n",
    "        def _normalize_weights(augmented_text_ids):\n",
    "            \"\"\"Normalize the weight of each position in a sequence.\"\"\"\n",
    "            for api_start_position in augmented_text_ids[\"api_start_positions\"].values():\n",
    "                total_weight = sum([seq_position[\"unnormalized_weight\"] for seq_position in api_start_position[\"seq_positions\"].values()])\n",
    "                for seq_position in api_start_position[\"seq_positions\"].values():\n",
    "                    seq_position[\"normalized_weight\"] = seq_position[\"unnormalized_weight\"] / total_weight\n",
    "            \n",
    "            return augmented_text_ids\n",
    "        \n",
    "        augmented_text_ids = _normalize_weights(augmented_text_ids)\n",
    "                \n",
    "        def extract_conditioning_ids_and_target_ids(augmented_text_ids):\n",
    "            conditioning_text_ids = torch.tensor([])\n",
    "            target_ids = torch.tensor([])\n",
    "            \n",
    "            for _, api_start_position_dict in augmented_text_ids[\"api_start_positions\"].items():\n",
    "                for _, seq_position_dict in api_start_position_dict[\"seq_positions\"].items():\n",
    "                    target_ids = torch.concat([target_ids, seq_position_dict[\"target_ids\"]], dim=0)\n",
    "                    for prompt_id in seq_position_dict[\"prompt_ids\"]:\n",
    "                        conditioning_text_ids = torch.cat([\n",
    "                            conditioning_text_ids,\n",
    "                            F.pad(prompt_id.long(), pad=(50-prompt_id.shape[-1], 0), value=self.pad_token_id).unsqueeze(0)\n",
    "                        ], dim=0)\n",
    "        \n",
    "            return conditioning_text_ids.long(), target_ids.long()\n",
    "\n",
    "        conditioning_text_ids, target_ids = extract_conditioning_ids_and_target_ids(augmented_text_ids)\n",
    "            \n",
    "        output = self.model(input_ids=conditioning_text_ids.long())\n",
    "        logits = output.logits[:, -1, :]\n",
    "                    \n",
    "        def extract_target_logprob_from_logits(logits, target_ids):\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            target_log_probs = log_probs[range(target_ids.shape[-1]), target_ids]\n",
    "            return target_log_probs\n",
    "\n",
    "        log_probs = extract_target_logprob_from_logits(logits, target_ids)\n",
    "            \n",
    "        for _, api_start_position_dict in augmented_text_ids[\"api_start_positions\"].items():\n",
    "            for _, seq_position_dict in api_start_position_dict[\"seq_positions\"].items():\n",
    "                seq_position_dict[\"losses\"] = log_probs[:3].squeeze(0)\n",
    "                log_probs = log_probs[3:]\n",
    "        \n",
    "        def _calculate_weighted_loss(augmented_text_ids):\n",
    "            for position in augmented_text_ids[\"api_start_positions\"]:        \n",
    "                seq_positions = augmented_text_ids[\"api_start_positions\"][position][\"seq_positions\"]\n",
    "                for i in seq_positions:\n",
    "                    losses = seq_positions[i][\"losses\"]\n",
    "                    weights = seq_positions[i][\"normalized_weight\"]\n",
    "                    seq_positions[i][\"weighted_losses\"] = -losses * weights\n",
    "            \n",
    "            return augmented_text_ids\n",
    "        \n",
    "        augmented_text_ids = _calculate_weighted_loss(augmented_text_ids)\n",
    "        \n",
    "        def _calculate_loss(augmented_text_ids):\n",
    "            data = {}\n",
    "            for position in augmented_text_ids[\"api_start_positions\"]:        \n",
    "                seq_positions = augmented_text_ids[\"api_start_positions\"][position][\"seq_positions\"]\n",
    "                losses = [0, 0, 0]            \n",
    "                for i in seq_positions:\n",
    "                    losses[0] += seq_positions[i][\"weighted_losses\"][0] # loss for [text]\n",
    "                    losses[1] += seq_positions[i][\"weighted_losses\"][1] # loss for [api->, text]\n",
    "                    losses[2] += seq_positions[i][\"weighted_losses\"][2] # loss for [api-result, text]\n",
    "                data[position] = losses\n",
    "                \n",
    "            return data\n",
    "        \n",
    "        losses = _calculate_loss(augmented_text_ids)\n",
    "        filtered_candidate_ids = self._filter_candidate_by_threshold(losses, candidate_ids)\n",
    "        return filtered_candidate_ids\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        text: str,\n",
    "    ) -> TensorType[\"n_apis\", \"n_candidates\", \"seq_len\"]:\n",
    "        filtered_apis = torch.tensor([])\n",
    "        \n",
    "        for api in self.apis:\n",
    "            # TODO: add support batch\n",
    "            prompt = api.prompt_template.format(input=text)\n",
    "            prompt_ids = self.tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        \n",
    "            # sampling positions\n",
    "            api_start_idxs, generated_ids = self.sample_api_position(prompt_ids)\n",
    "            \n",
    "            # obtaining api responses\n",
    "            candidate_ids = self.obtain_api_response(prompt_ids, api_start_idxs, generated_ids)\n",
    "\n",
    "            # filtering\n",
    "            text_ids = self.tokenizer(text, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "            \n",
    "            # return prompt_ids, api_start_idxs, generated_ids, candidate_ids, text_ids\n",
    "            filtered_candidate_ids = self.filter_api(api, text_ids, api_start_idxs, candidate_ids)\n",
    "                    \n",
    "            filtered_apis = torch.cat([filtered_apis, filtered_candidate_ids.unsqueeze(0)], dim=0)\n",
    "        \n",
    "        return filtered_apis.long()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
