[
  {
    "objectID": "00_prompt.html",
    "href": "00_prompt.html",
    "title": "toolformer",
    "section": "",
    "text": "::: {#826db027 .cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\nfrom nbdev.showdoc import *\n:::\n::: {#7a210c9e .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ncalculator_prompt = \"\"\"\nYour task is to add calls to a Calculator API to a piece of text. The API call should help you get information required to complete the text. \\n\nYou can call the API by writing \"Calculator(operation)!\" where \"operation\" is the type of calculation you want to perform. Here are some examples of API calls:\n\nInput: John has 5 apples and his friend gave him 3 more. John now has 8 apples.\nOuput: John has 5 apples and his friend gave him 3 more. John now has [Calculator(\"5 + 3\")] 8 apples.\n\nInput: Jane needs to divide 24 pieces of candy equally among 6 kids. Each kid will get 4 pieces of candy.\nOutput: Jane needs to divide 24 pieces of candy equally among 6 kids. Each kid will get [Calculator(24 / 6)] 4 pieces of candy.\n\nInput: From this, we have 4 * 30 minutes = 120 minutes.\nOutput: From this, we have 4 * 30 minutes = [Calculator(4 * 30)] 120 minutes.\n\nInput: {input}\nOutput:\n\"\"\"\n:::\n::: {#96099a20 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nqa_prompt = \"\"\"\nYour task is to add calls to a Question Answering API to a piece of text. The questions should help you get information required to complete the text.\nYou can call the API by writing \"[QA(question)]\" where \"question\" is the question you want to ask. Here are some examples of API calls:\n\nInput: Joe Biden was born in Scranton, Pennsylvania.\nOutput: Joe Biden was born in [QA(\"Where was Joe Biden born?\")] Scranton, [QA(\"In which state is Scranton?\")] Pennsylvania.\n\nInput: Coca-Cola, or Coke, is a carbonated soft drink manufactured by the Coca-Cola Company.\nOutput: Coca-Cola, or [QA(\"What other name is Coca-Cola known by?\")] Coke, is a carbonated soft drink manufactured by [QA(\"Who manufactures Coca-Cola?\")] the Coca-Cola Company.\n\nInput: {input}\nOutput:\n\"\"\"\n:::\n::: {#109306f4 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nwolframe_prompt = \"\"\"\nYour task is to add calls to a Scientific API to a piece of text that related to chemistry, math, physics. The questions should help you get information required to complete the text.\nYou can call the API by writing \"[Wolframe(question)]\" where \"question\" is the question you want to ask. Here are some examples of API calls:\n\nInput: The complex conjugate of 2 + 3i is 2 - 3i.\nOutput: The complex conjugate of 2 + 3i is [Wolframe(\"What is the complex conjugate of 2 + 3i\")] 2 - 3i.\n\nInput: Solve x^2 + 4x + 6 = 0. The answer is x = -2 - i sqrt(2)\nOutput: Solve x^2 + 4x + 6 = 0. The answer is [Wolframe(\"Solve x^2 + 4x + 6 = 0\")] x = -2 - i sqrt(2)\n\nInput: Given a sequence of numbers: 21.3, 38.4, 12.7, 41.6. The mean is 28.5\nOutput: Given a sequence of numbers: 21.3, 38.4, 12.7, 41.6. The mean is [Wolframe(\"What is the mean of 21.3, 38.4, 12.7, 41.6\")] 28.5\n\nInput: {input}\nOutput:\n\"\"\"\n:::",
    "crumbs": [
      "00_prompt.html"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "toolformer",
    "section": "",
    "text": "::: {#cell-1 .cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\n:::\nThis file will become your README and also the index of your documentation.",
    "crumbs": [
      "toolformer"
    ]
  },
  {
    "objectID": "index.html#developer-guide",
    "href": "index.html#developer-guide",
    "title": "toolformer",
    "section": "Developer Guide",
    "text": "Developer Guide\nIf you are new to using nbdev here are some useful pointers to get you started.\n\nInstall toolformer in Development mode\n# make sure toolformer package is installed in development mode\n$ pip install -e .\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to toolformer\n$ nbdev_prepare",
    "crumbs": [
      "toolformer"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "toolformer",
    "section": "Usage",
    "text": "Usage\n\nInstallation\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/qcname/toolformer.git\nor from conda\n$ conda install -c qcname toolformer\nor from pypi\n$ pip install toolformer\n\n\nDocumentation\nDocumentation can be found hosted on this GitHub repository’s pages. Additionally you can find package manager specific guidelines on conda and pypi respectively.",
    "crumbs": [
      "toolformer"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "toolformer",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n\n\n\nName\nPosition\n\n\n\n\nChris Lattner\nInventor of Swift and LLVM\n\n\n\nFernando Pérez\nCreator of Jupyter\n\n\n\nDavid Berg\nSoftware Engineer, Netflix\n\n\n\nErik Gaasedelen\nSoftware Engineer, Lyft\n\n\n\nRoxanna Pourzand\nProduct Manager, Transform\n\n\n\nHugo Bowne-Anderson\nHead of Developer Relations, Outerbounds\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that there are five types of callouts, including: note, warning, important, tip, and caution.",
    "crumbs": [
      "toolformer"
    ]
  },
  {
    "objectID": "00_utils.html",
    "href": "00_utils.html",
    "title": "toolformer",
    "section": "",
    "text": "::: {#e482eed6 .cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\nfrom nbdev.showdoc import *\n:::\n::: {#95a0148f .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nimport yaml\nimport re\nfrom typing import Optional\n:::\n::: {#fc79a808 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef yaml2dict(file_path):\n    with open(file_path,\"r\") as file:\n        data=yaml.safe_load(file)\n    return data\n:::\n::: {#8f6afd6a .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef extract_api_content(text:str,api_name:str)-&gt;str:\n    \"\"\"从给定的文本中抽取出api请求\"\"\"\n    start_tag=f\"{api_name}(\"\n    end_tag=\")\"\n    start_idx=text.find(start_tag)\n    if start_idx==-1:\n        return None\n    start_idx+=len(start_tag)\n    end_idx=text.find(end_tag,start_idx)\n    if end_idx==-1:\n        return None\n    return text[start_idx:end_idx]\n:::\n::: {#c5688fab .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef extract_api_syntax(text:str,api_name:str)-&gt;str:\n    \"\"\"抽取语法\"\"\"\n    pattern = r\"\\[{}\\(.*?\\)\\]\".format(api_name)\n    matches = re.findall(pattern, text)\n    return matches\n:::\n::: {#8578b362 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef extract_api_name(text:str,is_end_token:bool=True)-&gt;Optional[str]:\n    if is_end_token:\n        pattern=r'\\[(\\w+)\\(.+\\]\\s?'\n    else:\n        pattern=r'\\[(\\w+)\\(.+\\s?'\n    match=re.search(pattern,text)\n    if match:\n        return match.group(1)\n    else:\n        return None\n:::",
    "crumbs": [
      "00_utils.html"
    ]
  },
  {
    "objectID": "00_data_generator.html",
    "href": "00_data_generator.html",
    "title": "toolformer",
    "section": "",
    "text": "::: {#345402d9 .cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\nfrom nbdev.showdoc import *\n:::\n::: {#d637dc48 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nimport re\nfrom typing import List,Callable,Tuple,Union,TypedDict\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torchtyping import TensorType\nfrom einops import rearrange\nfrom toolformer.api import BaseApI\n:::\n::: {#20796764 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass AugmentedCandidate(TypedDict):\n    api_start_positions:int\n:::\n::: {#e3c6e019 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass DataGenerator(nn.Module):\n    def __init__(\n        self,\n        config: dict,\n        model: Callable, tokenizer: Callable,\n        apis: List[BaseApI],\n        #device: str = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        device=torch.device(\"cpu\")\n    ):\n        super().__init__()\n        start_character = config[\"data_generator\"][\"api_start_ch\"]\n        end_character = config[\"data_generator\"][\"api_end_ch\"]\n        output_character = config[\"data_generator\"][\"api_out_ch\"]\n        \n        # add a space, because when the model generate a token, it's also include a \"space\"\n        self.api_start_token_id = tokenizer(f' {start_character}', return_tensors=\"pt\")[\"input_ids\"][0]\n        self.api_end_token_id = tokenizer(end_character, return_tensors=\"pt\")[\"input_ids\"][0]\n        self.api_output_token_id = tokenizer(f'{output_character}', return_tensors=\"pt\")[\"input_ids\"][0]\n        \n        self.top_k_sampling = config[\"data_generator\"][\"top_k_sampling\"]\n        self.sampling_threshold = config[\"data_generator\"][\"sampling_threshold\"]\n        self.filtering_threshold = config[\"data_generator\"][\"filtering_threshold\"]\n        \n        self.apis = apis\n        self.model = model.to(device)\n        self.tokenizer = tokenizer\n        self.device = device\n        \n        # TODO: handle for cases that the sentence contains \".\\n\\n\"\n        self.pad_token_id = tokenizer.pad_token_id\n        self.eos_token_id = tokenizer(\".\\n\\n\")[\"input_ids\"][0]\n    \n    def sample_api_position(\n        self,\n        prompt_ids: TensorType[\"seq_len\"], # the ids of the prompt\n    ) -&gt; Tuple[\n        TensorType[\"n_positions\"], # The positions of api call\n        TensorType[\"seq_len\"] # The generated text\n    ]:\n        \"\"\"Sampling API positions.\"\"\"\n        # TODO: add support batch\n        # the ids of the prompt and generated_ids\n        prompt_and_generated_ids = prompt_ids\n        # only the ids of the generated_ids\n        generated_ids = torch.tensor([]).to(self.device)\n        i = torch.tensor([0]).to(self.device)\n        \n        api_pos_probs = torch.tensor([])\n        \n        with torch.no_grad():    \n            while True:\n                logits = self.model(\n                    input_ids=prompt_and_generated_ids.unsqueeze(0),\n                ).logits\n\n                last_logit = logits[0, -1, :]\n                probs = torch.softmax(last_logit, dim=-1)\n                api_start_prob = probs[self.api_start_token_id]\n                \n                if api_start_prob &gt; self.sampling_threshold:\n                    api_pos_probs = torch.cat([\n                        api_pos_probs,\n                        torch.tensor([api_start_prob, i]).unsqueeze(0)\n                    ], dim=0)     \n                \n                # sampling a token\n                # next_token = torch.multinomial(probs, num_samples=1)\n                next_token = torch.argmax(probs, dim=-1)\n                next_token = next_token.unsqueeze(0)\n                \n                prompt_and_generated_ids = torch.cat([prompt_and_generated_ids, next_token], dim=0)\n                generated_ids = torch.cat([generated_ids, next_token], dim=0)\n                \n                if next_token == self.eos_token_id:\n                    break\n                else:\n                    i += 1\n        \n        if api_pos_probs.numel() == 0:\n            api_positions = torch.tensor([])\n        else:\n            _, indices = torch.sort(api_pos_probs[:, 0], descending=True)\n            top_k_sampling = self.top_k_sampling\n            api_positions = api_pos_probs[indices[:top_k_sampling], 1]\n                    \n        return api_positions.long(), generated_ids.long()\n\n    def obtain_api_response(\n        self,\n        prompt_ids: TensorType[\"seq_len\"],\n        positions: TensorType[\"n_positions\"],\n        generated_ids: TensorType[\"seq_len\"]\n    ) -&gt; TensorType[\"n_positions\", \"seq_len\"]:\n        \n        MAX_PAD = 50\n        \n        # the ids before the start of an api call\n        pre_api_ids = torch.tensor([])\n\n        for position in positions:\n            text_ids = torch.cat([generated_ids[:position], self.api_start_token_id], dim=0)\n            padded_text_ids = F.pad(text_ids, pad=(MAX_PAD - text_ids.shape[-1], 0), value=self.pad_token_id)\n            \n            pre_api_ids = torch.cat([\n                pre_api_ids,\n                rearrange(padded_text_ids, \"... -&gt; 1 ...\")\n            ])\n        \n        PROMPT_LENGTH = len(prompt_ids)\n        \n        # TODO: optimzie this\n        prompt_and_pre_api_ids = torch.tensor([])\n        for x in pre_api_ids:\n            prompt_and_pre_api_ids = torch.cat([\n                prompt_and_pre_api_ids,\n                torch.cat([prompt_ids, x]).unsqueeze(0)\n            ], dim=0)\n                     \n        with torch.no_grad():\n            candidate_ids = self.model.generate(\n                input_ids=prompt_and_pre_api_ids.long(),\n                eos_token_id=self.eos_token_id,\n                max_new_tokens=50,\n            )\n        \n        # filter out the prompt template\n        # only keep the generated ids\n        candidate_ids = candidate_ids[:, PROMPT_LENGTH:]\n        \n        return candidate_ids\n    \n    def _generate_conditioning_prompts(\n        self,\n        api: BaseApI,\n        candidate_ids: TensorType[\"n_candidates\", \"seq_len\"],\n    ):\n        conditioning_api_ids = torch.tensor([])\n\n        API_NAME = api.name\n        MAX_PAD = 100\n        \n        def extract_api_request_content(text: str, api_name: str) -&gt; str:\n            \"\"\"Extract the content of an API request from a given text.\"\"\"\n            start_tag = f\"{api_name}(\"\n            end_tag = \")\"\n            start_idx = text.find(start_tag)\n            if start_idx == -1:\n                return None\n            start_idx += len(start_tag)\n            end_idx = text.find(end_tag, start_idx)\n            if end_idx == -1:\n                return None\n            return text[start_idx:end_idx]\n        \n        def extract_api_syntax(text: str, api_name: str) -&gt; str:\n            \"\"\"Extract the API Syntax from a given text.\"\"\"\n            pattern = r\"\\[{}\\(.*?\\)\\]\".format(api_name)\n            matches = re.findall(pattern, text)\n            return matches\n\n        for text_ids in candidate_ids:\n            # the ids of the prediction\n            text = self.tokenizer.decode(text_ids, skip_special_tokens=True)\n            \n            api_request_content = extract_api_request_content(text, api_name=API_NAME)\n            api_response = api(api_request_content)\n            api_response_ids = self.tokenizer(api_response, return_tensors=\"pt\")[\"input_ids\"][0]\n            # Format: \"-&gt; [api_response]\"\n            api_response_with_arrow_ids = torch.cat([self.api_output_token_id, api_response_ids], dim=0)\n            \n            api_syntax = extract_api_syntax(text, api_name=API_NAME)\n            api_syntax_ids = self.tokenizer(api_syntax, return_tensors=\"pt\")[\"input_ids\"][0]\n            api_syntax_with_response_ids = torch.cat([api_syntax_ids[:-1], api_response_with_arrow_ids, api_syntax_ids[-1:]])\n            api_syntax_without_response_ids = torch.cat([api_syntax_ids[:-1], self.api_output_token_id, api_syntax_ids[-1:]])\n                              \n            padded_api_without_response = rearrange(\n                F.pad(api_syntax_without_response_ids, pad=((MAX_PAD - api_syntax_without_response_ids.shape[-1]), 0), value=self.pad_token_id),\n                \"... -&gt; 1 ...\"\n            )\n            padded_api_with_response = rearrange(\n                F.pad(api_syntax_with_response_ids, pad=((MAX_PAD - api_syntax_with_response_ids.shape[-1]), 0), value=self.pad_token_id),\n                \"... -&gt; 1 ...\"\n            )\n        \n            padded_api_call = torch.cat([\n                padded_api_without_response,\n                padded_api_with_response\n            ], dim=0)\n            padded_api_call = rearrange(padded_api_call, \"... -&gt; 1 ...\")\n            \n            conditioning_api_ids = torch.cat([conditioning_api_ids, padded_api_call], dim=0).long()\n                    \n        return conditioning_api_ids\n\n    def _filter_candidate_by_threshold(\n        self,\n        losses,\n        candidates: TensorType[\"seq_len\"]\n    ):\n        filtered_augmented_text_ids = torch.tensor([])\n        for i, position in enumerate(losses):\n            negative_loss = min(losses[position][0], losses[position][1])\n            positive_loss = losses[position][2]\n            \n            if negative_loss - positive_loss &gt;= self.filtering_threshold:\n                # filtered_augmented_text_ids.append(candidates[i])\n                filtered_augmented_text_ids = torch.cat([\n                    filtered_augmented_text_ids,\n                    candidates[i].unsqueeze(0)\n                ], dim=0)\n        \n        return filtered_augmented_text_ids.long()\n\n    def filter_api( \n        self,\n        api: BaseApI,\n        text_ids: TensorType[\"seq_len\"],\n        api_start_idxs: TensorType[\"n_positions\"],\n        candidate_ids: TensorType[\"n_positions\", \"seq_len\"]\n    ):\n        conditioning_api_ids = self._generate_conditioning_prompts(api, candidate_ids)\n                \n        SPACE_TOKEN = self.tokenizer(\". \", return_tensors=\"pt\")[\"input_ids\"][0]\n        API_LENGTH = 100\n        augmented_text_ids = {\"api_start_positions\": {}}\n        \n        def _compute_weight(t: int) -&gt; Union[int, float]:\n            \"\"\"Compute the weight in the loss function.\"\"\"\n            return max(0, 1-0.2*t)\n        \n        for idx, api_ids in zip(api_start_idxs, conditioning_api_ids):\n            idx = idx.item()\n            seq_len = len(text_ids)\n            augmented_text_ids[\"api_start_positions\"][idx] = {\n                \"seq_positions\": {}\n            }\n\n            j = idx\n            while j &lt;= seq_len - 1:\n                # if the model predic\n                if j == 1:\n                    j += 1\n                    continue\n                \n                # in the formua, from x_1 to x_j (include x_j)\n                # =&gt; generate_ids[:j]\n                conditioning_text_ids = text_ids[:j]\n                api_and_text_ids = torch.stack([\n                    F.pad(conditioning_text_ids, pad=(API_LENGTH + len(SPACE_TOKEN), 0), value=self.pad_token_id), # [text_ids]\n                    torch.cat([api_ids[0], SPACE_TOKEN, conditioning_text_ids], dim=0), # [api-&gt;, text_ids]\n                    torch.cat([api_ids[1], SPACE_TOKEN, conditioning_text_ids], dim=0), # [api-&gt;result, text_ids]\n                ], dim=0)\n                                \n                # the next token after x_j\n                next_token_ids = text_ids[j]\n                augmented_text_ids[\"api_start_positions\"][idx][\"seq_positions\"][j] = {\n                    \"prompt_ids\": api_and_text_ids,\n                    \"unnormalized_weight\": _compute_weight(t=j-idx),\n                    \"losses\": [],\n                    \"target_ids\": torch.tensor([next_token_ids, next_token_ids, next_token_ids])\n                }\n                j += 1\n        \n        def _normalize_weights(augmented_text_ids):\n            \"\"\"Normalize the weight of each position in a sequence.\"\"\"\n            for api_start_position in augmented_text_ids[\"api_start_positions\"].values():\n                total_weight = sum([seq_position[\"unnormalized_weight\"] for seq_position in api_start_position[\"seq_positions\"].values()])\n                for seq_position in api_start_position[\"seq_positions\"].values():\n                    seq_position[\"normalized_weight\"] = seq_position[\"unnormalized_weight\"] / total_weight\n            \n            return augmented_text_ids\n        \n        augmented_text_ids = _normalize_weights(augmented_text_ids)\n                \n        def extract_conditioning_ids_and_target_ids(augmented_text_ids):\n            conditioning_text_ids = torch.tensor([])\n            target_ids = torch.tensor([])\n            \n            for _, api_start_position_dict in augmented_text_ids[\"api_start_positions\"].items():\n                for _, seq_position_dict in api_start_position_dict[\"seq_positions\"].items():\n                    target_ids = torch.concat([target_ids, seq_position_dict[\"target_ids\"]], dim=0)\n                    for prompt_id in seq_position_dict[\"prompt_ids\"]:\n                        conditioning_text_ids = torch.cat([\n                            conditioning_text_ids,\n                            F.pad(prompt_id.long(), pad=(50-prompt_id.shape[-1], 0), value=self.pad_token_id).unsqueeze(0)\n                        ], dim=0)\n        \n            return conditioning_text_ids.long(), target_ids.long()\n\n        conditioning_text_ids, target_ids = extract_conditioning_ids_and_target_ids(augmented_text_ids)\n            \n        output = self.model(input_ids=conditioning_text_ids.long())\n        logits = output.logits[:, -1, :]\n                    \n        def extract_target_logprob_from_logits(logits, target_ids):\n            log_probs = F.log_softmax(logits, dim=-1)\n            target_log_probs = log_probs[range(target_ids.shape[-1]), target_ids]\n            return target_log_probs\n\n        log_probs = extract_target_logprob_from_logits(logits, target_ids)\n            \n        for _, api_start_position_dict in augmented_text_ids[\"api_start_positions\"].items():\n            for _, seq_position_dict in api_start_position_dict[\"seq_positions\"].items():\n                seq_position_dict[\"losses\"] = log_probs[:3].squeeze(0)\n                log_probs = log_probs[3:]\n        \n        def _calculate_weighted_loss(augmented_text_ids):\n            for position in augmented_text_ids[\"api_start_positions\"]:        \n                seq_positions = augmented_text_ids[\"api_start_positions\"][position][\"seq_positions\"]\n                for i in seq_positions:\n                    losses = seq_positions[i][\"losses\"]\n                    weights = seq_positions[i][\"normalized_weight\"]\n                    seq_positions[i][\"weighted_losses\"] = -losses * weights\n            \n            return augmented_text_ids\n        \n        augmented_text_ids = _calculate_weighted_loss(augmented_text_ids)\n        \n        def _calculate_loss(augmented_text_ids):\n            data = {}\n            for position in augmented_text_ids[\"api_start_positions\"]:        \n                seq_positions = augmented_text_ids[\"api_start_positions\"][position][\"seq_positions\"]\n                losses = [0, 0, 0]            \n                for i in seq_positions:\n                    losses[0] += seq_positions[i][\"weighted_losses\"][0] # loss for [text]\n                    losses[1] += seq_positions[i][\"weighted_losses\"][1] # loss for [api-&gt;, text]\n                    losses[2] += seq_positions[i][\"weighted_losses\"][2] # loss for [api-result, text]\n                data[position] = losses\n                \n            return data\n        \n        losses = _calculate_loss(augmented_text_ids)\n        filtered_candidate_ids = self._filter_candidate_by_threshold(losses, candidate_ids)\n        return filtered_candidate_ids\n    \n    def generate(\n        self,\n        text: str,\n    ) -&gt; TensorType[\"n_apis\", \"n_candidates\", \"seq_len\"]:\n        filtered_apis = torch.tensor([])\n        \n        for api in self.apis:\n            # TODO: add support batch\n            prompt = api.prompt_template.format(input=text)\n            prompt_ids = self.tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"][0]\n        \n            # sampling positions\n            api_start_idxs, generated_ids = self.sample_api_position(prompt_ids)\n            \n            # obtaining api responses\n            candidate_ids = self.obtain_api_response(prompt_ids, api_start_idxs, generated_ids)\n\n            # filtering\n            text_ids = self.tokenizer(text, return_tensors=\"pt\")[\"input_ids\"][0]\n            \n            # return prompt_ids, api_start_idxs, generated_ids, candidate_ids, text_ids\n            filtered_candidate_ids = self.filter_api(api, text_ids, api_start_idxs, candidate_ids)\n                    \n            filtered_apis = torch.cat([filtered_apis, filtered_candidate_ids.unsqueeze(0)], dim=0)\n        \n        return filtered_apis.long()\n:::",
    "crumbs": [
      "00_data_generator.html"
    ]
  },
  {
    "objectID": "00_api.html",
    "href": "00_api.html",
    "title": "toolformer",
    "section": "",
    "text": "::: {#1f17f241 .cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\nfrom nbdev.showdoc import *\n:::\n::: {#d791788b .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nfrom abc import abstractmethod\nimport wolframalpha\nfrom langchain import PromptTemplate\nclass BaseApI:\n    def __init__(self,\n                 name: str, # the name of API CALL\n                 prompt_template:PromptTemplate,\n                 sampling_threshold: float=0.2,\n                 filtering_threshold: float=0.2):\n        self.name=name \n        self.prompt_template=prompt_template \n        self.sampling_threshold=sampling_threshold \n        self.filtering_threshold=filtering_threshold \n\n    @abstractmethod\n    def execute(self):\n        pass\n    def __call__(self,*args:str,**kargs:str) -&gt; str:\n        output=self.execute(*args,**kargs)\n        return str(output)\n:::\n::: {#fe49c16b .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass CalcuatorAPI(BaseApI):\n    def execute(self,input:str)-&gt;str:\n        try:\n            return eval(input)\n        except:\n            return \"\"\n:::\n::: {#63ab78d0 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass WolframeAPI(BaseApI):\n    def __init__(self,*args,api_key:str,**kargs):\n        super().__init__(*args,**kargs)\n        #self.api_key=api_key\n        self.api_key=\"4QR5HP-A8KLX2RG35\"\n    def execute(self,input:str)-&gt;str:\n        client=wolframalpha.Client(self.api_key)\n        res=client.query(input=input)\n        return next(res.results).text\n:::",
    "crumbs": [
      "00_api.html"
    ]
  },
  {
    "objectID": "00_model.html",
    "href": "00_model.html",
    "title": "toolformer",
    "section": "",
    "text": "::: {#1bd9f3f1 .cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\nfrom nbdev.showdoc import *\n:::\n::: {#35c24500 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nfrom typing import Optional, List\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom torchtyping import TensorType\nfrom einops import rearrange\n\nfrom toolformer.api import BaseApI\nfrom toolformer.utils import extract_api_content, extract_api_name\n:::\n::: {#34a1bc38 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass ToolFormer(nn.Module):\n    def __init__(\n        self,\n        model: AutoModelForCausalLM,\n        apis: List[BaseApI],\n        config: dict\n    ):\n        super().__init__()\n        self.model = model\n        self.apis = apis\n        self.config = config\n        self.is_calling_api: bool = False\n        \n        # TODO: make a config class contains token_id\n        tokenizer = AutoTokenizer.from_pretrained(self.config[\"tokenizer\"][\"path\"])\n        self.tokenizer = tokenizer # TODO: remove after debug\n        \n        start_character = config[\"data_generator\"][\"api_start_character\"]\n        end_character = config[\"data_generator\"][\"api_end_character\"]\n        output_character = config[\"data_generator\"][\"api_output_character\"]\n        \n        self.api_start_token_id = tokenizer(f' {start_character}', return_tensors=\"pt\")[\"input_ids\"][0]\n        self.api_end_token_id = tokenizer(end_character, return_tensors=\"pt\")[\"input_ids\"][0]\n        self.api_output_token_id = tokenizer(f'{output_character}', return_tensors=\"pt\")[\"input_ids\"][0]\n\n        self.eos_token_ids = tokenizer(\n            [\".\", \".\\n\\n\"],\n            return_tensors=\"pt\"\n        )[\"input_ids\"].squeeze()\n\n        # TODO: support batch\n        self.api_request_content: torch.Tensor = torch.tensor([])\n    \n    def _sampling(self, probs: TensorType[\"batch_size\", \"seq_len\"]) -&gt; TensorType[\"batch_size\", \"seq_len\"]:\n        return torch.argmax(probs, dim=-1)\n    \n    def execute_api(self, text_ids: TensorType[\"seq_len\"]) -&gt; Optional[TensorType[\"seq_len\"]]:\n        \"\"\"Execute an API call.\"\"\"\n        text = self.tokenizer.decode(text_ids, skip_special_tokens=True)\n        api_name = extract_api_name(text, is_end_token=False)\n\n        if api_name is not None:\n            # find does apis contains the api_name\n            for api in self.apis:\n                if api.name == api_name:\n                    api_content = extract_api_content(text, api_name=api_name)\n                    api_output = api(api_content)\n                    return self.tokenizer(api_output, return_tensors=\"pt\")[\"input_ids\"][0]\n        return None\n    \n    def add_idx_to_api_request_content(self, idx: TensorType[1]):\n        self.api_request_content = torch.cat([\n            self.api_request_content,\n            rearrange(idx, '... -&gt; 1 ...')\n        ], dim=-1).long()\n    \n    def forward(\n        self,\n        input_ids: TensorType[\"batch_size\", \"seq_len\"],\n        attention_mask: Optional[TensorType[\"batch_size\", \"seq_len\"]]=None,\n        max_new_tokens: int = 10,\n        **kwargs\n    ) -&gt; TensorType[\"batch_size\", \"seq_len\"]:\n        # check padding to the left\n        generated_ids = input_ids\n        \n        for _ in range(max_new_tokens):\n            output_ids = self.model(\n                input_ids=generated_ids,\n                attention_mask=attention_mask,\n                **kwargs\n            )\n            \n            logits = output_ids.logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            # TODO: k should be a config\n            _, top_k_idx = torch.topk(probs, k=1, dim=-1)\n            \n            if self.is_calling_api is True:\n                if self.api_end_token_id in top_k_idx:\n                    # if the api end token is in the top_k_idx, then we will execute the api\n                    # and then add api_end_token_id to the generated_ids\n                    # TODO: add support batch\n                    api_output_ids = self.execute_api(self.api_request_content[0])\n                    if api_output_ids is not None:\n                        pred_ids = torch.cat([\n                            self.api_output_token_id,\n                            api_output_ids,\n                            self.api_end_token_id\n                        ], dim=-1).long()\n                    else:\n                        pred_ids = self.api_end_token_id\n                    self.is_calling_api = False\n                else:\n                    pred_ids = self._sampling(probs)\n                    self.add_idx_to_api_request_content(pred_ids)\n            else:\n                if self.api_start_token_id in top_k_idx:\n                    # if the api start token is in the top_k_idx, then we are calling an api\n                    self.is_calling_api = True\n                    pred_ids = self.api_start_token_id\n                    self.add_idx_to_api_request_content(pred_ids)\n                else:\n                    pred_ids = self._sampling(probs)\n            \n            generated_ids = torch.cat([\n                generated_ids,\n                rearrange(pred_ids, '... -&gt; 1 ...')\n            ], dim=1)\n            \n            attention_mask = torch.cat([\n                attention_mask,\n                rearrange(torch.ones_like(pred_ids), '... -&gt; 1 ...')\n            ], dim=1)\n            \n            # ignore the case that pred_ids contains api_output\n            if len(pred_ids) == 1 and pred_ids in self.eos_token_ids:\n                break\n        \n        return generated_ids\n:::",
    "crumbs": [
      "00_model.html"
    ]
  }
]